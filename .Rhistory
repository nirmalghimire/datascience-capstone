twitter_quest<-grepl(u, twitterdata, ignore.case=TRUE)
twitter_doc<-twitterdata[twitter_quest]
k=length(twitter_doc)
if (k>0){
for( k in 1:k){
text_char[i+j+k]<-str_extract(twitter_doc[k], v)
YourNextWord[i+j+k]<-stri_extract_last_words(text_char[i+j+k])
}
}
creatingtable<-as.data.frame(YourNextWord, stringAsFactors=FALSE)
summary(creatingtable)
token_blog<-cleanfunction(creatingtable)
word_blog<-totalwords(token_blog)
summary(nchar(creatingtable))
head(creatingtable)
TDMatrix_blog<-TermDocumentMatrix(token_blog)
m_blog<-as.matrix(TDMatrix_blog)
sort_blog<-sort(rowSums(m_blog),decreasing=TRUE)
dataF_blog<-data.frame(word=names(sort_blog),freq=sort_blog)
head(sort_blog,100)
return(list(head(sort_blog,100)))
}
sentence1<-YourNextWord("a case of", "([Aa]+ + [Cc]ase+ + [Oo]f+ + [^ ]+ )")
sentence1<-YourNextWord("a case of", "([Aa]+ + [Cc]ase+ + [Oo]f+ + [^ ]+ )")
YourNextWord<-function(u,v){
blog_quest<-grepl(u, blogsdata, ignore.case=TRUE)
blog_doc<-blogsdata[blog_quest]
text_char<-'a'
YourNextWord<-'a'
i<-length(blog_doc)
if(i>0){
for (i in 1:i){
text_char[i]<-str_extract(blog_doc[i],v)
YourNextWord[i]<-stri_extract_last_words(text_char[i])
}
}
news_quest<-grepl(u, newsdata, ignore.case=TRUE)
news_doc<-newsdata[news_quest]
j=length(news_doc)
if (j>0){
for( j in 1:j){
text_char[i+j]<-str_extract(news_doc[j],v)
YourNextWord[i+j]<-stri_extract_last_words(text_char[i+j])
}
}
twitter_quest<-grepl(u, twitterdata, ignore.case=TRUE)
twitter_doc<-twitterdata[twitter_quest]
k=length(twitter_doc)
if (k>0){
for( k in 1:k){
text_char[i+j+k]<-str_extract(twitter_doc[k], v)
YourNextWord[i+j+k]<-stri_extract_last_words(text_char[i+j+k])
}
}
creatingtable<-as.data.frame(YourNextWord, stringAsFactors=FALSE)
summary(creatingtable)
token_blog<-cleanfunction(creatingtable)
word_blog<-totalwords(token_blog)
summary(nchar(creatingtable))
head(creatingtable)
TDMatrix_blog<-TermDocumentMatrix(token_blog)
m_blog<-as.matrix(TDMatrix_blog)
sort_blog<-sort(rowSums(m_blog),decreasing=TRUE)
dataF_blog<-data.frame(word=names(sort_blog),freq=sort_blog)
head(sort_blog,100)
return(list(head(sort_blog,100)))
}
sentence1<-YourNextWord("a case of", "([Aa]+ + [Cc]ase+ + [Oo]f+ + [^ ]+ )")
YourNextWord<-function(u,v){
blog_quest<-grepl(u, blogsdata, ignore.case=TRUE)
blog_doc<-blogsdata[blog_quest]
text_char<-'a'
YourNextWord<-'a'
i<-length(blog_doc)
if(i>0){
for (i in 1:i){
text_char[i]<-str_extract(blog_doc[i],v)
YourNextWord[i]<-stri_extract_last_words(text_char[i])
}
}
news_quest<-grepl(u, newsdata, ignore.case=TRUE)
news_doc<-newsdata[news_quest]
j=length(news_doc)
if (j>0){
for( j in 1:j){
text_char[i+j]<-str_extract(news_doc[j],v)
YourNextWord[i+j]<-stri_extract_last_words(text_char[i+j])
}
}
twitter_quest<-grepl(u, twitterdata, ignore.case=TRUE)
twitter_doc<-twitterdata[twitter_quest]
k=length(twitter_doc)
if (k>0){
for( k in 1:k){
text_char[i+j+k]<-str_extract(twitter_doc[k], v)
YourNextWord[i+j+k]<-stri_extract_last_words(text_char[i+j+k])
}
}
creatingtable<-as.data.frame(YourNextWord, stringAsFactors=FALSE)
summary(creatingtable)
token_blog<- cleanfunction(creatingtable)
word_blog<- totalwords(token_blog)
summary(nchar(creatingtable))
head(creatingtable)
TDMatrix_blog<-TermDocumentMatrix(token_blog)
m_blog<-as.matrix(TDMatrix_blog)
sort_blog<-sort(rowSums(m_blog),decreasing=TRUE)
dataF_blog<-data.frame(word=names(sort_blog),freq=sort_blog)
head(sort_blog,100)
return(list(head(sort_blog,100)))
}
sentence1<-YourNextWord("a case of", "([Aa]+ + [Cc]ase+ + [Oo]f+ + [^ ]+ )")
shiny::runApp('predicting_NG')
knitr::opts_chunk$set(echo = TRUE)
#Checking Most Frequent One Word, Two Words, Three Words, and 4 Words Combinations in the Combined Data Set
onegram<-function(x) NGramTokenizer(x, Weka_control(min=1, max=1))
onegram_tdm<-TermDocumentMatrix(corpus, control=list(tokenize=onegram))
corpus<-VCorpus(VectorSource(sample_together))
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(NLP)
library(tm)
library(slam)
library(xtable)
library(ngram)
library(RColorBrewer)
library(knitr)
library(corpus)
library(quanteda)
library(quanteda)
library(text2vec)
library(tidytext)
library(stringr)
library(spacyr)
bdata<-(file.info("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.blogs.txt")$size)/1024/1024
tdata<-(file.info("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.twitter.txt")$size)/1024/1024
ndata<-(file.info("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.news.txt")$size)/1024/1024
sprintf("The en_US.blogs.txt file is: %s Megabytes", bdata)
sprintf("The en_US.twitter.txt file is: %s Megabytes", tdata)
sprintf("The en_US.news.txt file is: %s Megabytes", ndata)
twitterdata<-file("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.twitter.txt", open="rb")
total_lines<- 0L
while(length(chunk<-readBin(twitterdata,"raw", 65536))>0){
total_lines<-total_lines+sum(chunk==as.raw(10L))
}
close(twitterdata)
blogsdata<-file("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.blogs.txt", open="rb")
total_lines0<-0L
while(length(chunk<-readBin(blogsdata,"raw", 65536))>0){
total_lines0<-total_lines0+sum(chunk==as.raw(10L))
}
close(blogsdata)
newsdata<-file("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.news.txt", open="rb")
total_lines1<-0L
while(length(chunk<-readBin(newsdata,"raw", 65536))>0){
total_lines1<-total_lines1+sum(chunk==as.raw(10L))
}
close(newsdata)
sprintf("The en_US.twitter.txt file has: %s Lines", total_lines)
sprintf("The en_US.blogs.txt file has: %s Lines", total_lines0)
sprintf("The en_US.news.txt file has: %s Lines", total_lines1)
blogsdata<-file("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.blogs.txt", open="rb")
twitterdata<-file("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.twitter.txt", open="rb")
newsdata<-file("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.news.txt", open="rb")
#Reading Lines
blogsdata_lines<-readLines(blogsdata, warn=FALSE, encoding="UTF-8")
close(blogsdata)
blogsdata_L<-summary(nchar(blogsdata_lines))[6]
blogsdata_L
twitterdata_lines<-readLines(twitterdata, warn=FALSE, encoding="UTF-8")
close(twitterdata)
twitterdata_L<-summary(nchar(twitterdata_lines))[6]
twitterdata_L
newsdata_lines<-readLines(newsdata, warn=FALSE, encoding="UTF-8")
close(newsdata)
newsdata_L<-summary(nchar(newsdata_lines))[6]
newsdata_L
set.seed(0916)
sample_blog<-sample(blogsdata_lines, length(blogsdata_lines)*0.1)
sample_twitter<-sample(twitterdata_lines, length(twitterdata_lines)*0.1)
sample_news<-sample(newsdata_lines, length(newsdata_lines)*0.1)
#Combining the samples and checking total length
sample_together<-c(sample_blog, sample_twitter, sample_news)
sample_together<-iconv(sample_together, "UTF-8", "ASCII", sub="")
length(sample_together)
corpus<-VCorpus(VectorSource(sample_together))
corpus<-tm_map(corpus, stripWhitespace)#Removes white spaces between words
corpus<-tm_map(corpus, content_transformer(tolower))#Converts texts or tokens to lower(or upper) case
corpus<-tm_map(corpus, removePunctuation)#Removes punctuation marks
corpus<-tm_map(corpus, removeNumbers)#Removes Numbers
corpus<-tm_map(corpus, PlainTextDocument)# Creates Plain Text Documents
corpus<-tm_map(corpus, removeWords, stopwords("english"))
#Checking Most Frequent One Word, Two Words, Three Words, and 4 Words Combinations in the Combined Data Set
onegram<-function(x) NGramTokenizer(x, Weka_control(min=1, max=1))
onegram_tdm<-TermDocumentMatrix(corpus, control=list(tokenize=onegram))
#Checking Most Frequent One Word, Two Words, Three Words, and 4 Words Combinations in the Combined Data Set
unigram<-function(x) NGramTokenizer(x, Weka_control(min=1, max=1))
unigram_tdm<-TermDocumentMatrix(corpus, control=list(tokenize=unigram))
install.packages("rJava")
install.packages(RWeka)
install.packages("RWeka")
$ sudo R CMD javareconf
install.packages("rJava",type = "source")
install.packages("RWeka")
install.packages("rJava", type = 'source')
install.packages("RWeka")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(NLP)
library(tm)
library(ngram)
library(RColorBrewer)
library(corpus)
library(stringi)
library(wordcloud)
library(RWeka)
options(mc.cores=1)
#Checking Most Frequent One Word, Two Words, Three Words, and 4 Words Combinations in the Combined Data Set
# A. Unigram
unigram<-function(x) NGramTokenizer(x, Weka_control(min=1, max=1))
unigram_tdm<-TermDocumentMatrix(corpus, control=list(tokenize=unigram))
unigram_HighFreq<-findFreqTerms(unigram_tdm, lowfreq=40)
# A.1. Calculating Unigram Frequency
freq_uni <- rowSums(as.matrix(unigram_tdm[unigram_HighFreq,]))
memory size()
memory.size()
gc()
gc()
memory.limit(6700)
memory.limit()
install.packages("devtools", dependencies = TRUE)
devtools::install_github("krlmlr/ulimit")
ulimit::memory_limit(2000)
memory.limit(2000)
knitr::opts_chunk$set(echo = TRUE)
# Calculating Unigram Frequency
frequency_unigram<-rowSums(as.matrix(tdm_uni[mostFreq_uni,]))
library(ggplot2)
library(NLP)
library(tm)
library(ngram)
library(RColorBrewer)
library(corpus)
library(stringi)
library(wordcloud)
library(RWeka)
options(mc.cores=1)
bdata<-(file.info("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.blogs.txt")$size)/1024/1024
tdata<-(file.info("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.twitter.txt")$size)/1024/1024
ndata<-(file.info("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.news.txt")$size)/1024/1024
sprintf("The en_US.blogs.txt file is: %s Megabytes", bdata)
sprintf("The en_US.twitter.txt file is: %s Megabytes", tdata)
sprintf("The en_US.news.txt file is: %s Megabytes", ndata)
twitterdata<-file("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.twitter.txt", open="rb")
total_lines<- 0L
while(length(chunk<-readBin(twitterdata,"raw", 65536))>0){
total_lines<-total_lines+sum(chunk==as.raw(10L))
}
close(twitterdata)
blogsdata<-file("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.blogs.txt", open="rb")
total_lines0<-0L
while(length(chunk<-readBin(blogsdata,"raw", 65536))>0){
total_lines0<-total_lines0+sum(chunk==as.raw(10L))
}
close(blogsdata)
newsdata<-file("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.news.txt", open="rb")
total_lines1<-0L
while(length(chunk<-readBin(newsdata,"raw", 65536))>0){
total_lines1<-total_lines1+sum(chunk==as.raw(10L))
}
close(newsdata)
sprintf("The en_US.twitter.txt file has: %s Lines", total_lines)
sprintf("The en_US.blogs.txt file has: %s Lines", total_lines0)
sprintf("The en_US.news.txt file has: %s Lines", total_lines1)
blogsdata<-file("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.blogs.txt", open="rb")
twitterdata<-file("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.twitter.txt", open="rb")
newsdata<-file("C:/Users/nirma/Documents/Coursera/Data Science/Data Science Capstone/final/en_US/en_US.news.txt", open="rb")
#Reading Lines
blogsdata_lines<-readLines(blogsdata, warn=FALSE, encoding="UTF-8")
close(blogsdata)
blogsdata_L<-summary(nchar(blogsdata_lines))[6]
blogsdata_L
twitterdata_lines<-readLines(twitterdata, warn=FALSE, encoding="UTF-8")
close(twitterdata)
twitterdata_L<-summary(nchar(twitterdata_lines))[6]
twitterdata_L
newsdata_lines<-readLines(newsdata, warn=FALSE, encoding="UTF-8")
close(newsdata)
newsdata_L<-summary(nchar(newsdata_lines))[6]
newsdata_L
set.seed(0916)
sample_blog<-sample(blogsdata_lines, length(blogsdata_lines)*0.05)
sample_twitter<-sample(twitterdata_lines, length(twitterdata_lines)*0.05)
sample_news<-sample(newsdata_lines, length(newsdata_lines)*0.05)
#Combining the samples and checking total length
sample_together<-c(sample_blog, sample_twitter, sample_news)
sample_together<-iconv(sample_together, "UTF-8", "ASCII", sub="")
length(sample_together)
corpus<-VCorpus(VectorSource(sample_together))
corpus<-tm_map(corpus, content_transformer(stripWhitespace))#Removes multiple white spaces between words
corpus<-tm_map(corpus, content_transformer(tolower))#Converts texts or tokens to lower(or upper) case
corpus<-tm_map(corpus, content_transformer(removePunctuation))#Removes punctuation marks
corpus<-tm_map(corpus, content_transformer(removeNumbers))#Removes Numbers
corpus<-tm_map(corpus, content_transformer(PlainTextDocument))# Creates Plain Text Documents
corpus<-tm_map(corpus, removeWords, stopwords("english"))
#Checking Most Frequent One Word, Two Words, Three Words, and 4 Words Combinations in the Combined Data Set
# A. UniGram
unigram<-function(x) NGramTokenizer(x,Weka_control(min=1,max=1))
tdm_uni<-TermDocumentMatrix(corpus,control=list(tokenize=unigram))
mostFreq_uni<-findFreqTerms(tdm_uni, lowfreq=40)
# B. BiGrams
bigram<-function(x) NGramTokenizer(x,Weka_control(min=2,max=2))
tdm_bi<-TermDocumentMatrix(corpus,control=list(tokenize=bigram))
mostFreq_bi<-findFreqTerms(tdm_bi, lowfreq=30)
# C. TriGrams
trigram<-function(x) NGramTokenizer(x,Weka_control(min=3,max=3))
tdm_tri<-TermDocumentMatrix(corpus,control=list(tokenize=trigram))
mostFreq_tri<-findFreqTerms(tdm_tri, lowfreq=20)
# D. QuadGrams
quadgram<-function(x) NGramTokenizer(x,Weka_control(min=4,max=4))
tdm_quad<-TermDocumentMatrix(corpus,control=list(tokenize=quadgram))
mostFreq_quad<-findFreqTerms(tdm_quad, lowfreq=15)
# Calculating Unigram Frequency
frequency_unigram<-rowSums(as.matrix(tdm_uni[mostFreq_uni,]))
# Calculating Unigram Frequency
memory.size(size=500000)
# Calculating Unigram Frequency
memory.size(500000)
frequency_unigram<-rowSums(as.matrix(tdm_uni[mostFreq_uni,]))
order_unigram<-order(frequency_unigram, decreasing=TRUE)
frequency_unigram<-data.frame(word=names(frequency_unigram[order_unigram]),frequency=frequency_unigram[order_unigram])
# Calculating Bigram Frequency
frequency_bigram<-rowSums(as.matrix(tdm_bi[mostFreq_bi,]))
order_bigram<-order(frequency_bigram, decreasing=TRUE)
frequency_bigram<-data.frame(word=names(frequency_bigram[order_bigram]),frequency1=frequency_bigram[order_bigram])
# Calculating Trigram Frequency
frequency_trigram<-rowSums(as.matrix(tdm_tri[mostFreq_tri,]))
order_trigram<-order(frequency_trigram, decreasing=TRUE)
frequency_trigram<-data.frame(word=names(frequency_trigram[order_trigram]),frequency2=frequency_trigram[order_trigram])
#Calculating Quadgram Frequency
frequency_quadgram<-rowSums(as.matrix(tdm_quad[mostFreq_quad,]))
order_quadgram<-order(frequency_quadgram, decreasing=TRUE)
frequency_quadgram<-data.frame(word=names(frequency_quadgram[order_quadgram]),frequency3=frequency_quadgram[order_quadgram])
# Bar Diagram for Unigrams
ggplot(frequency_unigram[1:25,], aes(factor(word, levels=unique(word)),frequency))+
geom_bar(stat='identity')+
theme(axis.text.x=element_text(angle=90))+
xlab("High Frequency Word List-30")+
ylab("Frequency")
# Bar Diagram for Bigrams
ggplot(frequency_biigram[1:25,], aes(factor(word, levels=unique(word)),frequency1))+
geom_bar(stat='identity')+
theme(axis.text.x=element_text(angle=90))+
xlab("High Frequency Word List-30")+
ylab("Frequency")
# Calculating Unigram Frequency
memory.size(500000)
frequency_unigram<-rowSums(as.matrix(tdm_uni[mostFreq_uni,]))
order_unigram<-order(frequency_unigram, decreasing=TRUE)
frequency_unigram<-data.frame(word=names(frequency_unigram[order_unigram]),frequency=frequency_unigram[order_unigram])
# Calculating Bigram Frequency
frequency_bigram<-rowSums(as.matrix(tdm_bi[mostFreq_bi,]))
order_bigram<-order(frequency_bigram, decreasing=TRUE)
frequency_bigram<-data.frame(word=names(frequency_bigram[order_bigram]),frequency1=frequency_bigram[order_bigram])
# Calculating Trigram Frequency
frequency_trigram<-rowSums(as.matrix(tdm_tri[mostFreq_tri,]))
order_trigram<-order(frequency_trigram, decreasing=TRUE)
frequency_trigram<-data.frame(word=names(frequency_trigram[order_trigram]),frequency2=frequency_trigram[order_trigram])
#Calculating Quadgram Frequency
frequency_quadgram<-rowSums(as.matrix(tdm_quad[mostFreq_quad,]))
order_quadgram<-order(frequency_quadgram, decreasing=TRUE)
frequency_quadgram<-data.frame(word=names(frequency_quadgram[order_quadgram]),frequency3=frequency_quadgram[order_quadgram])
# Bar Diagram for Unigrams
ggplot(frequency_unigram[1:25,], aes(factor(word, levels=unique(word)),frequency))+
geom_bar(stat='identity')+
theme(axis.text.x=element_text(angle=90))+
xlab("High Frequency Word List-30")+
ylab("Frequency")
# Bar Diagram for Bigrams
ggplot(frequency_biigram[1:25,], aes(factor(word, levels=unique(word)),frequency1))+
geom_bar(stat='identity')+
theme(axis.text.x=element_text(angle=90))+
xlab("High Frequency Word List-30")+
ylab("Frequency")
# Bar Diagram for Unigrams
ggplot(frequency_unigram[1:25,], aes(factor(word, levels=unique(word)),frequency))+
geom_bar(stat='identity')+
theme(axis.text.x=element_text(angle=90))+
xlab("High Frequency Word List-30")+
ylab("Frequency")
# Bar Diagram for Bigrams
ggplot(frequency_bigram[1:25,], aes(factor(word, levels=unique(word)),frequency1))+
geom_bar(stat='identity')+
theme(axis.text.x=element_text(angle=90))+
xlab("High Frequency Word List-30")+
ylab("Frequency")
# Word Cloud for Trigram
wordcloud(frequency_trigram$word, frequency_trigram$frequency2, max.words=20, colors=brewer.pal(8, "Set1"))
# Word Cloud for Quadgram
wordcloud(frequency_quadgram$word, frequency_quadgram$frequency3, max.words=20, colors=brewer.pal(8, "Set1"))
# Bar Diagram for Trigrams
ggplot(frequency_trigram[1:20,], aes(factor(word, levels=unique(word)),frequency2))+
geom_bar(stat='identity')+
theme(axis.text.x=element_text(angle=90))+
xlab("High Frequency Word List-30")+
ylab("Frequency")
# Bar Diagram for Quadgrams
ggplot(frequency_quadgram[1:20,], aes(factor(word, levels=unique(word)),frequency2))+
geom_bar(stat='identity')+
theme(axis.text.x=element_text(angle=90))+
xlab("High Frequency Word List-30")+
ylab("Frequency")
# Bar Diagram for Trigrams
ggplot(frequency_trigram[1:20,], aes(factor(word, levels=unique(word)),frequency2))+
geom_bar(stat='identity')+
theme(axis.text.x=element_text(angle=90))+
xlab("High Frequency Word List-30")+
ylab("Frequency")
# Bar Diagram for Quadgrams
ggplot(frequency_quadgram[1:20,], aes(factor(word, levels=unique(word)),frequency3))+
geom_bar(stat='identity')+
theme(axis.text.x=element_text(angle=90))+
xlab("High Frequency Word List-30")+
ylab("Frequency")
# Word Cloud for Unigram
wordcloud(frequency_unigram$word, frequency_unigram$frequency, max.words=20, colors=brewer.pal(8, "Set1"))
# Word Cloud for Bigram
wordcloud(frequency_bigram$word, frequency_bigram$frequency1, max.words=20, colors=brewer.pal(8, "Set1"))
# Calculating Unigram Frequency
memory.size(900000)
frequency_unigram<-rowSums(as.matrix(tdm_uni[mostFreq_uni,]))
order_unigram<-order(frequency_unigram, decreasing=TRUE)
frequency_unigram<-data.frame(word=names(frequency_unigram[order_unigram]),frequency=frequency_unigram[order_unigram])
# Calculating Bigram Frequency
frequency_bigram<-rowSums(as.matrix(tdm_bi[mostFreq_bi,]))
order_bigram<-order(frequency_bigram, decreasing=TRUE)
frequency_bigram<-data.frame(word=names(frequency_bigram[order_bigram]),frequency1=frequency_bigram[order_bigram])
# Calculating Trigram Frequency
frequency_trigram<-rowSums(as.matrix(tdm_tri[mostFreq_tri,]))
order_trigram<-order(frequency_trigram, decreasing=TRUE)
frequency_trigram<-data.frame(word=names(frequency_trigram[order_trigram]),frequency2=frequency_trigram[order_trigram])
#Calculating Quadgram Frequency
frequency_quadgram<-rowSums(as.matrix(tdm_quad[mostFreq_quad,]))
order_quadgram<-order(frequency_quadgram, decreasing=TRUE)
frequency_quadgram<-data.frame(word=names(frequency_quadgram[order_quadgram]),frequency3=frequency_quadgram[order_quadgram])
# Bar Diagram for Trigrams
ggplot(frequency_trigram[1:20,], aes(factor(word, levels=unique(word)),frequency2))+
geom_bar(stat='identity')+
theme(axis.text.x=element_text(angle=90))+
xlab("High Frequency Word List-30")+
ylab("Frequency")
# Bar Diagram for Quadgrams
ggplot(frequency_quadgram[1:20,], aes(factor(word, levels=unique(word)),frequency3))+
geom_bar(stat='identity')+
theme(axis.text.x=element_text(angle=90))+
xlab("High Frequency Word List-30")+
ylab("Frequency")
# Word Cloud for Unigram
wordcloud(frequency_unigram$word, frequency_unigram$frequency, max.words=20, colors=brewer.pal(8, "Set1"))
# Word Cloud for Bigram
wordcloud(frequency_bigram$word, frequency_bigram$frequency1, max.words=20, colors=brewer.pal(8, "Set1"))
# Calculating Unigram Frequency
memory.size(1500000)
frequency_unigram<-rowSums(as.matrix(tdm_uni[mostFreq_uni,]))
order_unigram<-order(frequency_unigram, decreasing=TRUE)
frequency_unigram<-data.frame(word=names(frequency_unigram[order_unigram]),frequency=frequency_unigram[order_unigram])
# Calculating Bigram Frequency
frequency_bigram<-rowSums(as.matrix(tdm_bi[mostFreq_bi,]))
order_bigram<-order(frequency_bigram, decreasing=TRUE)
frequency_bigram<-data.frame(word=names(frequency_bigram[order_bigram]),frequency1=frequency_bigram[order_bigram])
# Calculating Trigram Frequency
frequency_trigram<-rowSums(as.matrix(tdm_tri[mostFreq_tri,]))
order_trigram<-order(frequency_trigram, decreasing=TRUE)
frequency_trigram<-data.frame(word=names(frequency_trigram[order_trigram]),frequency2=frequency_trigram[order_trigram])
#Calculating Quadgram Frequency
frequency_quadgram<-rowSums(as.matrix(tdm_quad[mostFreq_quad,]))
order_quadgram<-order(frequency_quadgram, decreasing=TRUE)
frequency_quadgram<-data.frame(word=names(frequency_quadgram[order_quadgram]),frequency3=frequency_quadgram[order_quadgram])
# Calculating Unigram Frequency
memory.size(900000)
frequency_unigram<-rowSums(as.matrix(tdm_uni[mostFreq_uni,]))
order_unigram<-order(frequency_unigram, decreasing=TRUE)
frequency_unigram<-data.frame(word=names(frequency_unigram[order_unigram]),frequency=frequency_unigram[order_unigram])
# Calculating Bigram Frequency
frequency_bigram<-rowSums(as.matrix(tdm_bi[mostFreq_bi,]))
order_bigram<-order(frequency_bigram, decreasing=TRUE)
frequency_bigram<-data.frame(word=names(frequency_bigram[order_bigram]),frequency1=frequency_bigram[order_bigram])
# Calculating Trigram Frequency
frequency_trigram<-rowSums(as.matrix(tdm_tri[mostFreq_tri,]))
order_trigram<-order(frequency_trigram, decreasing=TRUE)
frequency_trigram<-data.frame(word=names(frequency_trigram[order_trigram]),frequency2=frequency_trigram[order_trigram])
#Calculating Quadgram Frequency
frequency_quadgram<-rowSums(as.matrix(tdm_quad[mostFreq_quad,]))
order_quadgram<-order(frequency_quadgram, decreasing=TRUE)
frequency_quadgram<-data.frame(word=names(frequency_quadgram[order_quadgram]),frequency3=frequency_quadgram[order_quadgram])
# Bar Diagram for Trigrams
ggplot(frequency_trigram[1:20,], aes(factor(word, levels=unique(word)),frequency2))+
geom_bar(stat='identity')+
theme(axis.text.x=element_text(angle=90))+
xlab("High Frequency Word List-30")+
ylab("Frequency")
# Bar Diagram for Quadgrams
ggplot(frequency_quadgram[1:20,], aes(factor(word, levels=unique(word)),frequency3))+
geom_bar(stat='identity')+
theme(axis.text.x=element_text(angle=90))+
xlab("High Frequency Word List-30")+
ylab("Frequency")
# Word Cloud for Unigram
wordcloud(frequency_unigram$word, frequency_unigram$frequency, max.words=20, colors=brewer.pal(8, "Set1"))
# Word Cloud for Bigram
wordcloud(frequency_bigram$word, frequency_bigram$frequency1, max.words=20, colors=brewer.pal(8, "Set1"))
shiny::runApp('predicting_NG')
